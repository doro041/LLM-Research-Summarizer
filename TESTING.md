# ðŸ§ª Testing Guide

## Quick Test

Run this to verify everything works:

```bash
python -c "from app.services.citation_analyzer import CitationNetworkAnalyzer; from app.services.qa_system import ResearchQASystem; print('âœ… All imports successful!')"
```

## Component Tests

### 1. Test Citation Analyzer

```python
from app.services.citation_analyzer import CitationNetworkAnalyzer, Citation
from langchain.schema import Document

# Create test data
analyzer = CitationNetworkAnalyzer()
test_doc = Document(page_content="""
Recent work by Smith et al. (2023) and Jones (2022) has shown 
promising results. Building on the transformer architecture 
(Vaswani et al., 2017), we propose a novel approach.
""")

# Extract citations
citations = analyzer.extract_citations_from_documents([test_doc])
print(f"Found {len(citations)} citations")

# Analyze
stats = analyzer.analyze_citations(citations)
print(f"Total: {stats.total_citations}")
print(f"Unique: {stats.unique_citations}")

# Generate report
report = analyzer.generate_citation_report(stats)
print(report[:200])
```

**Expected Output:**
```
Found 3 citations
Total: 3
Unique: 3
# ðŸ“š Citation Network Analysis

## Overview
- **Total Citations**: 3
- **Unique Citations**: 3
```

### 2. Test Q&A System

```python
from app.services.qa_system import ResearchQASystem
from langchain.schema import Document

qa_system = ResearchQASystem()

test_docs = [
    Document(page_content="""
    Methodology: We used a transformer-based architecture with 
    12 layers and 768 hidden dimensions. The model was trained 
    on 100GB of text data for 3 days on 8 V100 GPUs.
    """),
    Document(page_content="""
    Results: Our model achieved 92% accuracy on the test set, 
    outperforming the previous state-of-the-art by 3 percentage 
    points. The model showed particular strength on long documents.
    """)
]

# Test single question
result = qa_system.answer_question(
    "What methodology did the authors use?", 
    test_docs
)
print(f"Q: {result['question']}")
print(f"A: {result['answer'][:100]}...")
print(f"Confidence: {result['confidence']}")
```

**Expected Output:**
```
Q: What methodology did the authors use?
A: The authors used a transformer-based architecture with 12 layers and 768 hidden dimensions. Th...
Confidence: high
```

### 3. Test Full Pipeline

Create a test script:

```python
# test_pipeline.py
import os
from app.services.citation_analyzer import CitationNetworkAnalyzer
from app.services.qa_system import ResearchQASystem
from app.loaders.pdf_loader import load_and_split

def test_pipeline(pdf_path):
    print("Testing full pipeline...")
    
    # Load PDF
    print("1. Loading PDF...")
    documents = load_and_split(pdf_path)
    print(f"   âœ“ Loaded {len(documents)} chunks")
    
    # Citation analysis
    print("2. Analyzing citations...")
    analyzer = CitationNetworkAnalyzer()
    citations = analyzer.extract_citations_from_documents(documents)
    stats = analyzer.analyze_citations(citations)
    print(f"   âœ“ Found {stats.total_citations} citations")
    
    # Q&A
    print("3. Running Q&A...")
    qa_system = ResearchQASystem()
    result = qa_system.answer_question("What are the main contributions?", documents)
    print(f"   âœ“ Answer generated (confidence: {result['confidence']})")
    
    # Research gaps
    print("4. Identifying research gaps...")
    gaps = qa_system.generate_research_gaps(documents)
    print(f"   âœ“ Found {len(gaps)} opportunities")
    
    print("\nâœ… All tests passed!")

if __name__ == "__main__":
    test_pipeline("data/your_paper.pdf")
```

Run with:
```bash
python test_pipeline.py
```

## Integration Tests

### Test Example Scripts

```bash
# Test single paper analysis
python examples/analyze_paper.py data/test_paper.pdf

# Test interactive mode (type 'quit' to exit)
python examples/analyze_paper.py -i data/test_paper.pdf
```

### Test Main Script

```bash
# Update main.py with your PDF path
python main.py
```

**Check for these outputs:**
- `summary.md` âœ“
- `summary.tex` âœ“
- `summary.docx` âœ“
- `citation_analysis.md` âœ“
- `citation_analysis.tex` âœ“
- `qa_analysis.md` âœ“
- `qa_analysis.tex` âœ“
- `research_gaps.md` âœ“

## Common Issues & Solutions

### Issue: "Import could not be resolved"
**Solution:** Install dependencies
```bash
pip install -r requirements.txt
```

### Issue: "API Key Error"
**Solution:** Check `.env` file
```bash
echo GOOGLE_API_KEY=your_key_here > .env
```

### Issue: "No citations found"
**Solution:** Some papers use non-standard citation formats. This is expected for some documents.

### Issue: "Rate limit exceeded"
**Solution:** The system automatically handles this with retries. If persistent, wait 1 minute.

### Issue: "PDF not found"
**Solution:** Check the file path is correct
```python
# In main.py
pdf_path = "data/your_paper.pdf"  # Must exist
```

## Performance Benchmarks

Test with different paper sizes:

| Paper Pages | Chunks | Citations | Processing Time | API Calls |
|-------------|--------|-----------|-----------------|-----------|
| 5-10        | 15-30  | 20-40     | ~1-2 min        | 25-35     |
| 10-20       | 30-60  | 40-80     | ~2-3 min        | 35-50     |
| 20-30       | 60-90  | 80-120    | ~3-5 min        | 50-70     |

## Unit Tests (Optional)

Create `tests/test_citation_analyzer.py`:

```python
import pytest
from app.services.citation_analyzer import CitationNetworkAnalyzer, Citation

def test_citation_extraction():
    analyzer = CitationNetworkAnalyzer()
    text = "Recent work (Smith, 2023) shows promise."
    citations = analyzer._extract_citations_from_text(text, "Test")
    assert len(citations) > 0
    assert citations[0].authors == "Smith"
    assert citations[0].year == 2023

def test_temporal_distribution():
    analyzer = CitationNetworkAnalyzer()
    years = [2015, 2018, 2020, 2023]
    dist = analyzer._calculate_temporal_distribution(years)
    assert dist["2015_2020"] == 2
    assert dist["2020_present"] == 2
```

Run tests:
```bash
pytest tests/
```

## Validation Checklist

Before deploying:

- [ ] All imports work
- [ ] API key configured
- [ ] Test PDF loads successfully
- [ ] Citations extracted (even if 0 found)
- [ ] Q&A generates answers
- [ ] Research gaps identified
- [ ] All export formats generate
- [ ] Interactive mode works
- [ ] Example scripts run
- [ ] No crashes on error cases

## Debug Mode

Add to config.py for verbose output:

```python
DEBUG = True
VERBOSE_LOGGING = True
```

Then in your code:
```python
from app.config import DEBUG

if DEBUG:
    print(f"Debug: Processing chunk {i}/{total}")
```

---

**All systems ready for research paper analysis!** ðŸš€
